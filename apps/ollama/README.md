# Ollama

Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.

### Variants

- `intel`: Supports Intel Graphcis with SYCL Support\
  URI: `ghcr.io/futursolo/portable-ai/ollama:intel`

  **IPEX-LLM Optimised Build of Ollama**
  Base Repository: https://github.com/intel/ipex-llm

### Usage

See: `[variant]/docker-compose.example.yml`
