---
name: example-ollama

services:
  ollama:
    build:
      dockerfile: ./ollama/intel/Dockerfile
      context: ../..
    image: ghcr.io/futursolo/pai-apps/ollama:intel
    devices:
      - /dev/dri
    shm_size: 16G
    environment:
      - ZES_ENABLE_SYSMAN=1
      - SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1

      # Single GPU
      - ONEAPI_DEVICE_SELECTOR=level_zero:0

      # Multiple GPUs
      # - ONEAPI_DEVICE_SELECTOR="level_zero:0;level_zero:1"
    group_add:
      - video
      - $GROUP_ID_RENDER  # See README.md about how to obtain this ID.
    networks:
      - app_tier
    restart: unless-stopped

    # WSL Only
    privileged: true

    volumes:
      # WSL Only
      - /usr/lib/wsl:/usr/lib/wsl

      - read_only: false
        source: ../../../test-data/ollama
        target: /mnt/ollama
        type: bind

  # Pull models with the following command:
  # docker compose -f docker-compose.example.yml run -it --rm ollama-pull tinyllama
  # If you have a default docker-compose.yml file, you can pull models with:
  # docker compose run --rm -it ollama-pull tinyllama
  ollama-pull:
    profiles:
      - extras
    build:
      dockerfile: ./ollama/intel/Dockerfile
      context: ../..
    image: ghcr.io/futursolo/pai-apps/ollama:intel
    entrypoint: ["/opt/ollama-extras/ollama-safe-pull.sh"]

    volumes:
      - read_only: false
        source: ../../../test-data/ollama
        target: /mnt/ollama
        type: bind

  # We accept traffic with the ingress container and forward it to the app.
  ingress:
    image: alpine/socat:latest
    depends_on:
      ollama:
        condition: service_started
    ports:
      - 11434:11434
    networks:
      - app_tier
      - ingress_tier
    command: "TCP-LISTEN:11434,fork,reuseaddr TCP4:ollama:11434"

networks:
  app_tier:
    internal: true
  ingress_tier:
