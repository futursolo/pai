---
name: example-llama-cpp

services:
  llama-cpp:
    build:
      dockerfile: ./llama.cpp/rocm/Dockerfile
      context: ../..
    image: ghcr.io/futursolo/pai-apps/llama.cpp:rocm
    devices:
      - /dev/dri
      - /dev/kfd
    environment:
      HSA_OVERRIDE_GFX_VERSION: 11.0.0
      PYTORCH_ROCM_ARCH: gfx1100
    command: >
      --server
      -m /mnt/models/Llama-3.3-70B-Instruct-Q4_K_M.gguf
      -c 2048 -ngl 999 -mg 0
      --port 8080
      --host 0.0.0.0
    group_add:
      - video
      - render
    ports:
      - 8080:8080
    restart: unless-stopped
    security_opt:
      - seccomp:unconfined
    volumes:
      - read_only: true
        source: ../../../test-models
        target: /mnt/models
        type: bind
